{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    '''\n",
    "    :argument inputDimension: the number of expecting inputs into the network\n",
    "    :argument hiddenLayers: a list defining the dimensions of the hidden layers\n",
    "                [lyr1, lyr2, lyr3, .... lyrN]\n",
    "    '''\n",
    "    def __init__(self, input_dimension, hidden_layer_nodes, output_dimension):\n",
    "        self.learningRate = 0.01\n",
    "        self.weights = []\n",
    "        #self.zetas = []\n",
    "        self.bias = []\n",
    "        self.outputs = []\n",
    "        self.inputDimension = input_dimension\n",
    "        self.outputDimension = output_dimension\n",
    "\n",
    "        self.inputLayer = np.array([[0 for _ in range(input_dimension)]], dtype=float).transpose()\n",
    "        prev_dimension = input_dimension\n",
    "        for node in hidden_layer_nodes:\n",
    "            self.bias.append(np.array([[random.random() for _ in range(node)]], dtype=float).transpose())\n",
    "            self.weights.append(\n",
    "                np.array([random.random() for _ in range(node * prev_dimension)],\n",
    "                         dtype=float).reshape(node, prev_dimension)\n",
    "            )\n",
    "            prev_dimension = node\n",
    "\n",
    "        self.bias.append(np.array([[random.random() for _ in range(output_dimension)]], dtype=float).transpose())\n",
    "        self.weights.append(np.array(\n",
    "            [random.random() for _ in range(prev_dimension * output_dimension)],\n",
    "            dtype=float).reshape(output_dimension, prev_dimension))\n",
    "\n",
    "\n",
    "    '''\n",
    "    :argument input: an arry of size n defined in the constructor\n",
    "    '''\n",
    "    def predict(self, input_arr):\n",
    "        output = None\n",
    "        self.outputs.clear()\n",
    "        self.inputLayer = np.array([input_arr],dtype=float).transpose()\n",
    "        activate = np.vectorize(self.activate)\n",
    "        current_input = np.copy(self.inputLayer)\n",
    "        for i, weight_matrix in enumerate(self.weights):\n",
    "            zeta = np.matmul(weight_matrix, current_input,) + self.bias[i]\n",
    "            #self.zetas.append(zeta)\n",
    "            output = activate(zeta)\n",
    "            self.outputs.append(output)\n",
    "            current_input = output\n",
    "\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def activate(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def dactivate(x):\n",
    "        return NeuralNetwork.activate(x) * (1 - NeuralNetwork.activate(x))\n",
    "    @staticmethod\n",
    "    def compute_gradient(results_vector, desired_vector, zeta):\n",
    "        sigma_ddx = np.vectorize(NeuralNetwork.dactivate)\n",
    "        ddZeta = sigma_ddx(zeta)\n",
    "        gradient = (results_vector - desired_vector) * ddZeta\n",
    "        return gradient\n",
    "    \n",
    "    def train(self, input_arr, target):\n",
    "        #print(input_arr)\n",
    "        target = np.array([target],dtype=float).transpose()\n",
    "        self.predict(input_arr)\n",
    "        sigma_ddx = np.vectorize(NeuralNetwork.dactivate)\n",
    "        output = self.outputs.pop()\n",
    "        error = target - output\n",
    "        indicies = [i for i in reversed(range(len(self.weights))) ]\n",
    "        for index in indicies:\n",
    "            gradient = error * sigma_ddx(output) * self.learningRate\n",
    "            self.bias[index] += gradient\n",
    "            try:\n",
    "                deltas = np.matmul(gradient, self.outputs[-1].transpose())\n",
    "            except:\n",
    "                deltas = np.matmul(gradient, self.inputLayer.transpose())\n",
    "                self.weights[index] += deltas\n",
    "                return\n",
    "            #print('weights[{}]\\n{}\\nDeltas:\\n{}'.format(index,self.weights[index],deltas))\n",
    "            self.weights[index] += deltas\n",
    "            error = np.matmul(self.weights[index].transpose(), error)\n",
    "            output = self.outputs.pop()\n",
    "            \n",
    "    def dump_network(self):\n",
    "        for i, matrix in enumerate(self.weights):\n",
    "            print('Weight Matrix[{}]\\n{}'.format(i,matrix))\n",
    "            print('Bias[{}]\\n{}'.format(i,self.bias[i]))\n",
    "            print('Node[{}]\\n{}'.format(i,self.outputs[i]))\n",
    "           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2, [1,1,2,3,5,8,13,21], 2)\n",
    "answerkey = [True,False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerkey[nn.predict([1,0]).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.dump_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'target':[0,1], 'input':[0,0]},\n",
    "        {'target':[1,0], 'input':[0,1]},\n",
    "        {'target':[1,0], 'input':[1,0]},\n",
    "        {'target':[0,1], 'input':[1,1]}\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100000):\n",
    "    for d in data:\n",
    "        random.shuffle(data)\n",
    "        nn.train(d['input'],d['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answerkey[nn.predict([1,1]).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.dump_network()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
